\documentclass[a4paper,titlepage,12pt]{article}                               
%Title, date and author                                                
\def\xtitle{A case study in CUDA programming:\\Running a statistical test on a distribution of galaxies}
%Header labels with the same (or different, if you change it) text as the title 
\def\xheadertitle{\xtitle}                                                      
\def\xauthor{Oskar Lappi, 37146}
\def\xdate{February 2020}
              
%Highlighting
\usepackage{tcolorbox}
\usepackage{xcolor}
%E.g. pandas tables
\usepackage{booktabs}
%Maths
\usepackage{amssymb}
\usepackage{amsmath}
%Fonts
\usepackage{helvet}
%Layout and graphics
\usepackage{graphicx}
\usepackage[margin=2cm]{geometry}
\usepackage{lastpage}
\usepackage{fancyhdr}
%Source code
\usepackage{listings}
%Lists
\usepackage{enumitem}
%Links
\usepackage{hyperref}

\hypersetup{
        pdfborder={0 0 0},
        colorlinks=true,
        linkcolor=black,
        urlcolor=cyan
        }


%Header & footer
\setlength{\headheight}{13.6pt}
\pagestyle{fancyplain}
\fancyhf{}
\lhead{ \fancyplain{}{\xtitle}}
\rhead{ \fancyplain{}{\xauthor}}
\rfoot{ \fancyplain{}{}\thepage /\pageref{LastPage}}

%Paragraphs
\edef\xparskip{\the\parskip\relax}
\edef\xparindent{\the\parindent\relax}

\setlength{\parskip}{0pt}


%Paragraph set macros
\def\setpar (#1 #2){
\setlength{\parskip}{#1\relax}           
\setlength{\parindent}{#2\relax}
}

\edef\resetpar{
\setlength{\parskip}{\the\parskip\relax}
\setlength{\parindent}{\the\parindent\relax}
}


%Margin

%Title
\title{\xtitle}
\date{\xdate}
\author{\xauthor}

%Font
\renewcommand{\familydefault}{phv}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%% MACROS %%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%Images
\newcommand{\img}[3][0.95]{
        \begin{figure}[h]
                \centering
                \includegraphics[width=#1\textwidth]{img/#2}
                \caption{#3}
                \label{fig:#2}
        \end{figure}
}

%Semantic typography

\definecolor{flagcol}{rgb}{0.92, 0.800, 0.630}
\definecolor{codecol}{rgb}{0.9, 0.9, 0.9}
\definecolor{listingbg}{rgb}{0.95, 0.95, 0.95}
\definecolor{keywordcol}{rgb}{0.01, 0.12, 0.67}
\definecolor{keywordcol2}{rgb}{0.68, 0.12, 0.47}
\definecolor{keywordcol3}{rgb}{0.012, 0.58, 0.27}
\definecolor{keywordcol4}{rgb}{0.012, 0.58, 0.57}

\newtcbox{\code}{on line,colframe=codecol,colback=codecol,boxrule=0.5pt,arc=4pt,left=6pt,right=6pt,top=3pt,bottom=3pt,boxsep=0pt}
\newtcbox{\flags}{on line,colframe=flagcol, colback=flagcol,boxrule=0.5pt,arc=4pt,left=6pt,right=6pt,top=3pt,bottom=3pt,boxsep=0pt,}

%Q & A macro
\def\QnA Q:#1 A:#2|||{
\begin{description}
        \item [Q:] {\bfseries #1}
        \item [A:] #2
\end{description}
}

%Code listing
\lstset{ 
backgroundcolor=\color{listingbg},
language=C,
morekeywords={__shared__,__global__,uint64_t,uint32_t,uint16_t},keywordstyle=\color{keywordcol},
classoffset=1,
morekeywords={galactic_coordinate,cart_coordinate,cart_coordinate_SoA32,BIN_TYPE},keywordstyle=\color{keywordcol2},
classoffset=2,
morekeywords={ARCMIN_CONV_RATE,GOOD_ENOUGH_PI,RAD2DEG_CONV_RATE,RAD2QUARTDEG_CONV_RATE,ANGLE_PER_THREAD,BIN_DIM,SET_DIM},keywordstyle=\color{keywordcol3},
classoffset=3,
morekeywords={blockDim,blockIdx,threadIdx},keywordstyle=\color{keywordcol4},
captionpos=b,
breakatwhitespace=true,
breaklines=true,
frame=single,
framexleftmargin=3mm,
framexbottommargin=3mm,
framextopmargin=3mm,
%frame=shadowbox,
%framerule=0.1cm,
%rulesepcolor=\color{grey},
%frameround=tttt,
}

\begin{document}
\maketitle

\section*{Preface}

This report is written for the 2019-2020 course {\em GPU Programming} at Åbo Akademi, organized by Jan Westerholm.
The assignment is to write a CUDA program that calculates a statistic measure on two distributions of coordinates.
The coordinates happen to be galactic coordinates and the result may support the existence of dark matter in the universe, but this problem context is not the object of study --- the purpose of the assignment is to write an efficient program that runs on a GPU.

I chose to write a theoretical {\bf Part I} so that I could better reason about the program and make educated decisions while designing it.
Just jump straight to {\bf part III} if you're familiar with CUDA and interested in the work and results.

\vspace{0.5cm}
\noindent The report is divided into four parts:
\begin{description}[left = \parindent]
\item [Part I] describes the CUDA programming model.
\item [Part II] describes the assignment, the data to be processed, and the work that the \\program has to do.
\item [Part III] contains all the practical work, including the final design, steps I took to get there, and the results.
\item [Part IV] contains my final thoughts and conclusions, I try to derive some rules of thumb from the work.

\end{labeling}

\subsection*{Definitions}
\noindent In this report, I often use CUDA as synonymous with the CUDA C++ language extensions.
E.g. when I write "CUDA program" I refer to a program written in C++ with CUDA extensions.
The report is written with CUDA Compute Capability 7.0 in mind.

\subsection*{Typographic conventions}
\begin{itemize}
\item Terms and titles are in {\em italic} the first time they appear
\item Text in a CUDA program appears \code{like this}
\item Programs and command line options appear \flags{like this}
\end{itemize}

\clearpage
\part{The CUDA programming model}

\section{Dramatis personæ: The host and the device}

The CUDA programming model is {\em heterogeneous}.
This means the processors that a CUDA program runs on may have different architectures.

A CUDA program runs on one processor (the CPU), which is called the {\em host}.
Coprocessors (typically GPUs) to the host are called {\em devices}.
In a CUDA program, the host is explicitly instructed by the programmer to run program segments on a device.
The work given to the devices typically constists of the heaviest workloads in the program --- the host holds the reins and the devices pull the cart.

%TODO:You can do better!
The host and device are logically separated from each other.
They have separate symbol tables, they operate on separate instruction streams, they even have separate memory.

\subsection{Kernels}

The segments of a CUDA program that will run on a device are called {\em kernels}.
A kernel is defined as a function decorated with the \code{\_\_global\_\_} {\em execution space specifier} that will be called from the host.

\subsection{Asynchronous execution}
\label{ssec:async}
Memory transfers and computation on the host and device use resources which are independent from each others'.
To use this to our advantage, the CUDA programming model allows for organizing memory management and execution calls into {\em streams}.
All operations within a stream are performed synchronously, but operations in separate streams are (essentially) asynchronous and can execute .
I used streams but they weren't terribly useful.

\subsection{Runtime of a typical CUDA program}
A CUDA program begins its runtime on the host with a call to main, just like any other C++ program.
The host manages memory on both itself and devices.
The host may also copy memory from itself to the host and vv.
In any nontrivial CUDA program, the host will launch a kernel on the device, specifying how many threads should be executed with which block and grid dimensions.%Runtime? Compile time? Guessing runtime

\newpage
\section{Units of computation}
%Source: CUDA Toolkit Documentation

\subsection{Warp}

%Warp = like a weaving machine?
A warp is a set of 32 threads (on modern NVidia GPUs), and it is generally the smallest unit of threads worth reasoning about.
The warp differs from other units of computation in that it is managed by the hardware, not the programmer.

Every thread in a warp is on an individual {\em warp lane}, an ID ranging from 0 to 31.
The lane is the threads 1D index modulo 32.
The 1D index is calculated as \(z_i|X\times Y|+ y_i|X| + x_i\).

Threads in a warp used to be run \underline{simultaneously} on the GPU on a single multiprocessor, but they are now independent as of compute capability 7.0. %https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#independent-thread-scheduling-7-x
This is not the only way that threads in a warp are special, a GPU programmer can use {\em warp level programming} to create a more efficient program.
E.g. memory access patterns, as I discuss in \ref{sssec:gmem_warp} and \ref{ssec:warp_mem}.

%Clean up^


\subsection{Thread blocks}

Threads are logically grouped into blocks.
Every thread in a block has a 3D block-index.
The dimensions of the block index are up to the programmer to decide (1D and 2D indexing spaceѕ have a widht of 1 for superfluous dimensions).
All threads in a block witll be local to one multiprocessor.
Threads within a block can share memory through {\em shared memory}, and synchronize at low cost with \code{\_\_syncthreads()}.
Threads within a block are executed in any order, but one warp at a time.

\subsection{Grids}

Blocks are further grouped into a grid.
A block has a 3D index within a grid (just like threads do within blocks).
Blocks within a grid can be executed in any order.

\subsection{Cooperative groups}

Since CUDA 9.0, threads can also be synchronized at other levels than thread blocks.
A programmer can define a cooperative group of threads within or across thread blocks.
The functionality is exposed in the header \code{\(<\)cooperative\_groups.h\(>\)}.

\noindent We can start from the block or the grid by creating a cooperative group out of them:

\begin{itemize}
	\item \code{thread\_block b = this\_thread\_block();}
	\item \code{grid\_group g = this\_grid();}
\end{itemize}

\noindent The thread block can then be split into smaller groups of 8 with a call to:
\begin{center}
	\code{thread\_group tile = tiled\_partition(b,8)}
\end{center}

\noindent (I found little reference to whether a grid can be split into smaller pieces.)
These structures offer a unified interface for synchronization and indexing of threads within a group.
The interface of \code{thread\_group} is fairly simple:
\begin{itemize}
	\item \code{sync()} to synchronize the group
	\item \code{thread\_rank()} gives a unique id within the group from 0 to size -1
	\item \code{size()} returns the size of the group
\end{itemize}

\newpage
\section{Physical memory layout}

The useful distinction of where memory is physically mapped are: {\em off-chip} and {\em on-chip}.
Both physical memory spaces are on the device, but on-chip memory is on the same chip as the multiprocessors.
On-chip memory can additionally be split (on a physical level) into registers and shared memory.

In addition to this split there are on-chip (per multiprocessor) caches that make reads quicker.
\begin{itemize}
	\item constant, read only cache
	\item unified data cache (128KB for Volta)
	\begin{itemize}
		\item L1
		\item textures
		\item shared memory
	\end{itemize}
	\item L2 cache
\end{itemize}

\section{Globally scoped memory}

Device memory can be categorized into two different classes:
\begin{itemize}
	\item Dynamically allocated, globally scoped memory. Off-chip (but may be cached on-chip)
	\item Statically allocated, locally scoped memory. On-chip (except for local memory)
\end{itemize}

In general the program must allocate memory to the global, constant, and texture memory segments.
Shared memory, registers, and local memory can be statically determined at compile time.
%Check validity

I first discuss globally scoped segments of memory, and then the locally scoped ones.

Global memory is read write; constant and texture memory is read only.


\subsection{Global memory}

Global memory is the biggest chunk of memory available.
It is also in the slowest category.

Global memory is cached in L2.

\subsubsection{Global memory access within a warp}

\label{sssec:gmem_warp}

Global memory operations within a warp are {\em coalesced}. %CUDA Toolkit best practices 9.2.1
For compute capability \(\geq\) 6.0, memory access will be grouped into 32-byte transactions. Even though L1-caching is enabled by default, transactions will still be units of 32 bytes.

\subsection{Constant memory}

Constant memory is off-chip read-only memory.
Constant memory is cached in the {\em constant cache}, but cache misses will read all the way from the off-chip device memory.
Constant memory is a good candidate when all threads in a warp access the same location in constant memory, because reads within a warp are only performed once for each unique address in constant memory.
Such cases are arguments to kernels, and constants that can't be put in instructions.
It's mostly used by the compiler, but programmers can use it to load one or two constants into if they want to.

Constant memory reads are however \underlined{at best} the same cost as register reads, and there is rather little of it available (some 64 kB), so it is a bad substitute for global memory.
Best to leave constant memory to the compiler.

\subsection{Texture memory}

The caching of texture memory in the {\em unified data cache} is 2D-spatially local.
Texture memory can be preferrable to global memory when processing 2D-spatially local data.
I did not conѕider using texture memory for this assignment.

\newpage
\section{Locally scoped memory}

Memory within kernel or device functions are locally scoped, and have a lifetime limited by the lifetime of the scope.
Unlike the persistent pointers allocated by globally scoped memory, these memory segments are on-chip (except for local memory, which is mapped to off-chip memory).

%Some memory will be mapped to 

\subsection{Single thread scope}

All declared variables in a kernel or device function will be scoped to a single thread, unless they are declared \code{\_\_shared\_\_}.

\subsubsection{Registers}

Registers are the fastest memory available, generally requiring \underline{0 clock cycles}.%Best practices 9.2.6
Registers will be partitioned along concurrent threads.
The amount of registers allocated per thread can be controlled using the \flags{-maxrregcount=N} option to nvcc, or the \code{\_\_launch\_bounds\_\_()} qualifier.
The register count per thread will limit the number of blocks per multiprocessor.
More blocks is better (because it minimizes warp latency), but more register use is better locally (because it's faster).

\subsubsection{Local memory}

Local memory is not locally situated, it is off chip (on the device but not near a multiprocessor).
This means local memory is slower than other types of locally scoped memory.

Since this is true, local memory is only allocated in special cases.
The compiler will attempt to use registers as much as it can, but in some cases, it will allocate to local (off-chip) memory:
\begin{itemize}
	\item non-constant sized arrays will be put in local memory
	\item if there aren't enough registers left, variables will be put in local memory ({\em register spilling})
\end{itemize}
% Programming guide 5.3.2 Device memory access

Since this behaviour is implicit, and we want to minimize it, it is best to check.
You can measure the usage of local memory, as described in section \ref{sec:meas_mem}.


\subsection{Shared memory}

Shared memory has to be explicitly declared by a programmer with the \code{\_\_shared\_\_} decorator.
Shared memory is grafted out of the on-chip {\em unified data cache} (in fact, CUDA refers to it as a {\em carveout} from the unified data cache).
The remainder of the udc is used as and L1 cache.

\subsubsection{Bank conflicts}

Shared memory is organized into banks, which can be simultaneously accessed.
But access to a single bank by many theads will be serialized.
Banks are spread out 32-bit.

\subsection{Warp level memory constructs}
\label{ssec:warp_mem}
While not explicit, there is a warp level memory space.
A program can perform operations on registers used by a warp using {\em warp shuffle functions} and {\em warp matrix functions}.

These operations use registers assigned accross warp lanes in a coordinated way.
\begin{center}
\code{T \_\_shfl\_up\_sync(unsigned mask, T var, unsigned int delta)}
\end{center}
will return the value of \code{var} in the lane with id \code{delta} less than the current lane id.
These accesses happen \underline{simultaneously}.
Since registers are quicker than shared memory, this is a faster way to share memory accross warps.

\section{Checking and measuring memory use}
\label{sec:meas_mem}
\subsection{From source code}
The so called {\em address space predicate functions} can be used determine the memory space of a pointer.
The all have the signature \code{unsigned int …(const void *ptr)}
\begin{itemize}
	\item \code{\_\_isGlobal}
	\item \code{\_\_isShared}
	\item \code{\_\_isConstant}
	\item \code{\_\_isLocal}
\end{itemize}
%B.13 programming guide

\subsection{From the compiler}

nvcc reports total local memory usage, register usage, shared memory usage, etc. when run using the \flags{--ptxas-options=-v}.
\newpage
\section{Making things parallel}

\subsection{Atomic functions}

CUDA provides atomic functions for performing arithmetic and logical operations atomically.
These operations are atomic because they lock the address being modified in the operation.
These operations may be atomic on a block, device, or system level.

\subsection{Overlap}

As I described in section \ref{ssec:async}, host, kernels, and memcpy are asynchronous with regard to each other.
If we had a lot of data transfer to do, we could interleave that with kernel runs.
We can also interleave host and device operations, running them in parallel.
To do this we may have to allocate pinned memory and use streams.

\clearpage
\part{The problem}

\section{Background}

The hypothesis of the existence of dark matter in the universe has been made because traditional astrophysical models fo not explain astrophysical observations.
Therefore --- the astrophysicists argue --- there must be some unobserved and hitherto unobservable (or dark) quantities that cause the observable quantities in the universe to behave unlike what the astrophysical models predict.

We are tasked with running a statistical test to support this claim, using empirically measured and randomly generated.
The specific models and measurements we will consider are:

\begin{itemize}
	\item models of gravitation
	\item the position of galaxies
\end{itemize}

If the models and initial state are valid, the two data sets should be "similar".
If not, then either the models are wrong, or the modeled state is incomplete.
Since there is supporting evidence for the models being right, it's more likely that there is some quantities missing from the state. (This is my understanding of the logic of the argument)

So, what does the data look like, and what does "similar" mean?
We look at these in sections \ref{sec:data} and \ref{sec:stat_test} respectively.

\section{Data}
\label{sec:data}

There are two data sets with 100 000 coordinates each.
One contains real {\bf D}ata, the other {\bf R}andomly generated data.
We call these the {\bf D} and {\bf R} data sets.

The program will have to load these from a file when the program is run.
The data in both consists of galactic coordinates in the form of right ascension and declination.
I will transform this data into cartesian 3D points.

\paragraph{My program will also exploit} the fact that the points all lie within 90\(^\circ\) of each other.

\section{The work}
\label{sec:stat_test}

We will consider the cartesian products: \(D\times D\), \(R\times R\), and \(D\times R\).
For each set of pairs, we will calculate the angle between each pair and generate the histograms: {\bf DD},{\bf RR}, {\bf DR}. Each histogram should have a resolution of a quarter of a degree.

To test the hypothesis, we will calculate a statistical measure, which is part of a statistical test.
I don't know what the test actually {\em is}, or the test statistic, the statistic test in the context of this report simply determines the work that has to be done.


The statistical test described above is calculated as:

\[\omega_i = (DD_i - 2DR_i + RR_i)/RR_i) \]

If \(omega_i\) is greater than, say, 0.5, this means that the distributions are dissimilar for the bin \(i\).

The greater part of the work will be calculating the angles and histograms.
Calculating the test statistic is several orders of magnitude simpler.

\clearpage
\part{Design, Implementation and validation}

\section{Overview}

The overview of the program is as follows:
\begin{enumerate}
	\item load data from files
	\item transform into cartesian coordinates
	\item move data to the GPU
	\item calculate angles between pair of coordinates
	\item increment the corresponding histogram bin for each angle
	\item move data from the GPU
	\item calculate the statistical measure
\end{enumerate}

\vspace{0.5cm}
\noindent List item 4 and 5 on the above list are performed on the device, while the rest is done on the host.
This is because there are 30 000 000 000 angles to calculate, while there are only 200 000 coordinates, and 1080 bins.

\subsection{Process}

The version which I present to you is about the third iteration.
I had some trouble early on getting good performance out of the program, but I am somewhat satisfied with this version.

I used \flags{nvcc} to compile the program, \flags{time} to time the program, \flags{nvprof} to profile, and I tried to debug with \flags{cuda-gdb}, but that didn't work so well.
In order to profile host code, I used the NVIDIA Tools Extension Library.
I used make and git to organize my work on dione, which is where I eventually did all of my development.
My makefile is on the next page.

\newpage
\begin{lstlisting}[caption=the project makefile]
CXX=/usr/local/cuda/bin/nvcc
CPPFLAGS=-lm -arch=compute_70 -code=sm_70,sm_72 -lineinfo --ptxas-options=-v -lnvToolsExt
srun_flags=-p gpu -o log/out.log -e log/error.log
jobid_find=s/.*job \([[:digit:]]*\) queued.*/\1/p
REAL=data/real/data_100k_arcmin.txt
FAKE=data/real/flat_100k_arcmin.txt

all : load_cuda darkmatter

darkmatter: src/darkmatter.cu src/*.h
        $(CXX) $(CPPFLAGS) -o $@ src/darkmatter.cu

.PHONY: run
run: darkmatter
        @mkdir -p log
        $(eval PROG_ID=$(shell /bin/bash -c "srun $(srun_flags) $^ $(REAL) $(FAKE) 2>&1 | tee /dev/tty |sed -n '$(jobid_find)'"))
        @./report_run.sh $(PROG_ID)

.PHONY: debug
debug: darkmatter
        @mkdir -p log
        srun -p gpu --pty cuda-gdb --args $^ $(REAL) $(FAKE)

.PHONY: time
time: darkmatter
        srun -p gpu --pty time ./$^ $(REAL) $(FAKE)

.PHONY: prof
prof: darkmatter
        srun -p gpu --pty nvprof ./$^ $(REAL) $(FAKE)
        
.PHONY: vprof
vprof: darkmatter
        srun -p gpu --pty nvprof -f -o darkmatter.prof --cpu-profiling on ./$^ $(REAL) $(FAKE)

.PHONY : clean
clean: 
        rm -rf build
        rm -f src/**/*.o
        rm -rf  log
\end{lstlisting}

%Improvements?
% Use restricted pointers to __possibly__ reduce the # of instructions
% - https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#restrict
% Use Profiler counter functions to drill down on ¶erformance 
%
\section{Analysis}

\subsection{What device is available to us}
Using the cudaGetDeviceProperties function from the runtime API, we get the following information about a GPU partition on the SLURM cluster dione.utu.fi
\begin{verbatim}
Device Number: 0
  Compute capability version:   7.0
  Device name:                  Tesla V100-PCIE-16GB
  Memory Clock Rate (KHz):      877000
  Memory Bus Width (bits):      4096
  Peak Memory Bandwidth (GB/s): 898.048000


  Total global memory:          16945512448 bytes
  Total constant memory:        65536 bytes
  L2 cache size:                6291456 bytes
  Warpsize:                     32
  32bit regs per block:         65536
  Shared memory per block:      49152 bytes

  Max #threads per block:       1024
  Max block dims (in threads):  1024 x 1024 x 64
  Max grid size:                2147483647 x 65535 x 65535
  #Multiprocessors:             80
  #Threads per multiprocessor:  2048

  ECC:                          1
  Concurrent kernel support:    1
  Unified addressing:           1
  Memcpy/kernel concurrency support(asyncEngineCount): 7
----
Device 0 can access device 0's memory = 0
Device 0 can access device 1's memory = 1
Device 0 can access device 2's memory = 1
Device 0 can access device 3's memory = 1
\end{verbatim}

\noindent These numbers were very useful to me when I designed the program.
\subsection{Optimal numbers}

\subsubsection{Histogram size}

As I alluded to earlier, the maximum angle between two coordinates in the datasets was 90 degrees.
This means we only need a histogram with \(90\cdot 4 = 360\) bins.

\subsubsection{Block size}

For each of the histograms, we are calculating an angle for each pair in \(Z_{10^5}\times Z_{10^5}\).
Luckily for us \(10^5 \mid 2^5\) and \(10^{10} \mid 2^{10}\), and \(2^{10} = 1024\) happens to be the maximum number of threads in a block.
Since \(2^{10}\) is a square, I will make my blocks 2-dimensional, 32 by 32.
This can be used for an elegant mapping from memory to computation.


\subsubsection{\# of registers}
There is a total of 65536 registers.
We have 1024 threads in a block.
Each thread can therefore use at most \(65536/1024 = 64\) registers, and preferably 32, 21, or 16, since this would allow us to pack 2 and 4 blocks into one multiprocessor respectively.
Values just underneath these numbers would also be ok.
%measure ^

I have used a single kernel.
The kernel both calculates angles and increments the corresponding bins.
Each thread calculates one angle and increments one bin.
The same kernel is run three times, with the arguments (D,D), (R,R), and (D,R).

\subsection{Memory}

We can imagine the cartesian product of the two datasets as a 100 000 by 100 000 matrix.
Since my blocks are 32 by 32, we can map the blocks to 32 by 32 submatrices of this matrix.
A subset of threads can fetch a value each from global memory.
This means each block only has to fetch 64 values from global memory, and they can all be shared within the block to calculate 1024 results.

These 1024 results will be atomically added to a histogram within the block.
Finally the block level histogram is added to a histogram in global memory.

We are performing more adds in total, but fewer to global memory by a factor of 1024.


\subsection{Warp level}

The most important thing to consider on a warp level for this program is that threads within a warp not diverge, as this can slow down the overall performance of the program.

Since the blocks are 32 by 32, the warp is identified by \code{threadIdx.y}, and the lane by \code{threadIdx.x}.
If my ifs use \code{threadIdx.y} as the condition, and my loops contain as many whole warps as possible, the situation is optimal.
\newpage
\section{Design and Implementation}

I list the interesting bits of the program below.
First, definitions and host code, then device code.
The host is responsible for:
\begin{itemize}[noitemsep]
	\item Preprocessing
	\item The calculation of the statistic
	\item File I/O (trivial, not presented)
	\item Validation (not profiled, not presented) %But should it be presented?
\end{itemize}


\subsection{Macros and definitions}

\begin{lstlisting}
#define GOOD_ENOUGH_PI 3.14159
#define ARCMIN_CONV_RATE 0.000291
#define RAD2DEG_CONV_RATE 57.296
#define RAD2QUARTDEG_CONV_RATE 229.184

#define BIN_DIM 360
\end{lstlisting}

\begin{lstlisting}
typedef struct galactic_coordinate {
  float right_ascension;
  float declination;
} galactic_coordinate;

typedef struct cart_coordinate {
  float x;
  float y;
  float z;
} cart_coordinate;

typedef unsigned long long BIN_TYPE;
\end{lstlisting}


\noindent\code{BIN\_TYPE} is typedefed because I didn't want to type \code{uint64\_t} or \code{unsigned long long} every time I refered to it.
It also makes it easier to switch types, should the size of the dataset change.
\newpage
\subsection{Preprocessing}

Preprocessing takes care of transforming the coordinates from spherical coordinates to cartesian coordinates.

\begin{lstlisting}
void preprocess_cart(
  galactic_coordinate coords_in[],
  cart_coordinate coords_out[],
  int n_coords)
{
  int i;
  for (i = 0;i < n_coords; i++) {

    float phi = coords_in[i].right_ascension*ARCMIN_CONV_RATE;
    float theta = 
      GOOD_ENOUGH_PI/2 - coords_in[i].declination*ARCMIN_CONV_RATE;

    float sin_theta = sinf(theta);

    coords_out[i].x = sin_theta*cosf(phi);
    coords_out[i].y = sin_theta*sinf(phi);
    coords_out[i].z = cosf(theta);
  }
}
\end{lstlisting}

\subsubsection{Calculating the statistic}

\begin{lstlisting}
void calc_kolmogorov(
  double *kolm_smir,
  BIN_TYPE *dr,
  BIN_TYPE *dd,
  BIN_TYPE *rr)
{
  for (int i = 0; i < 360; i++){
    kolm_smir[i] = ((double)dd[i] - 2*(double)dr[i] + (double)rr[i])/(double)rr[i];
    if (kolm_smir[i] > 10){
      printf("Bin %d suspiciously high: (%lld - 2*(%lld) + %lld)/%lld = %lf\n",
        i,dd[i],dr[i],rr[i],rr[i],kolm_smir[i]);
    }
  }
}
\end{lstlisting}

\newpage
\subsection{The kernel}

\begin{lstlisting}
__global__ void calc_DR_bins(
  cart_coordinate *D,
  cart_coordinate *R,
  BIN_TYPE *bins)
{
  uint64_t t_id_in_block = threadIdx.y*blockDim.x+threadIdx.x;
  uint64_t x_angle_start = blockDim.x*blockIdx.x*ANGLE_PER_THREAD;
  uint64_t y_angle_start = blockDim.y*blockIdx.y*ANGLE_PER_THREAD;
  __shared__ uint32_t block_bins[BIN_DIM];
  __shared__ cart_coordinate shD[ANGLE_PER_THREAD*32];
  __shared__ cart_coordinate shR[ANGLE_PER_THREAD*32];

  if (threadIdx.y == 31)
    for (int i = threadIdx.x;
    	 i < 32*ANGLE_PER_THREAD && x_angle_start+i < 100000;i+=32
    )
      shD[i] = D[x_angle_start+i];
  else if (threadIdx.y == 30)
    for (int i = threadIdx.x;
    	 i < 32*ANGLE_PER_THREAD && y_angle_start+i < 100000;i+=32
    )
      shR[i] = R[y_angle_start+i];
  else if (t_id_in_block < BIN_DIM)
    block_bins[t_id_in_block] = 0;
  

  __syncthreads();
  for(int i = threadIdx.x;
      i <  32*ANGLE_PER_THREAD && x_angle_start+i < 100000;i+=32){
    for(int j = threadIdx.y;
        j < 32*ANGLE_PER_THREAD && y_angle_start+j < 100000;j+=32){

      uint16_t bin_id =(uint16_t) (
	acosf(
	  shD[i].x*shR[j].x+
	  shD[i].y*shR[j].y+
	  shD[i].z*shR[j].z
	)*RAD2QUARTDEG_CONV_RATE);
      bin_id = MIN(bin_id,359);
      atomicAdd_block(&block_bins[bin_id],1);
  }}

  __syncthreads();

  if (t_id_in_block < BIN_DIM)
    atomicAdd(&bins[t_id_in_block],block_bins[t_id_in_block]);
}
\end{lstlisting}

Reads are written to be optimally coalesced in warps (and for warps not to diverge), but it's done in two warps only, since I don't know ahead of time how many angles per thread there are.
Also, since the reads are conditioned on which warp a thread is in warps won't diverge, which is more efficient.
The condition for clearing a bin is not exactly warp-specific, there will be one warp with mixed branching, but overall the condition will map to as many whole warps as possible.

By changing the \code{ANGLE\_PER\_THREAD} constant, we can decide how many angles are run per thread.
Per thread means how many angles are \underline{fetched} per thread, every thread actually calculates \(ANGLE\_PER\_THREAD^2\) angles.

Every thread block is mapped to a segment in the complete matrix of pairs \code{D} \(\times\) \code{R} consisting of \(ANGLE\_PER\_THREAD^2\) counts of \(32 \times 32\) submatrices.
And every thread maps to an angle in the same position in each submatrix.
The coordinate in the submatrix is the \code{threadIdx.x} for \code{D} and the \code{threadIdx.y} for \code{R}.

\subsubsection{Writes}

After the reading is done, every thread calculates its bins, and increments the thread blocks shared histogram.
At the end, the same threads that zeroed the shared histogram will add the shared histogram to the histogram in global memory, one thread at a time.

\subsection{The main function}

\begin{lstlisting}[caption=The main function]
int main(int argc, char *argv[]) {
  //...Allocations left out for legibility
  unsigned int grid_len =((3125)+ANGLE_PER_THREAD-1)/ANGLE_PER_THREAD;
  dim3 grid_dims(grid_len,grid_len);
  dim3 block_dims(32,32);

  cudaStream_t stream[2];
  for (int i = 0; i < 2; ++i)
    cudaStreamCreate(&stream[i]);

  //Read one file
  galactic_coordinate *raw_coords =
    (galactic_coordinate *) calloc(SET_DIM,sizeof(galactic_coordinate));
  read_dataset(raw_coords,argv[1],SET_DIM);
  preprocess_cart(raw_coords,real_coords,SET_DIM);

  //Start DD-calculation
  cudaMemcpyAsync(
  	d_real_coords,real_coords,
	SET_DIM*sizeof(cart_coordinate),
	cudaMemcpyHostToDevice,
	stream[0]);
  calc_DR_bins<<<grid_dims, block_dims,0,stream[0]>>>
    (d_real_coords,d_real_coords,d_dd_bins);
  //...

  //...

  //While the first kernel runs, read second file
  read_dataset(raw_coords,argv[2],SET_DIM);
  preprocess_cart(raw_coords,real_coords+SET_DIM,SET_DIM);

  //..and start the RR-calculation
  cudaMemcpyAsync(
    d_fake_coords,real_coords+SET_DIM,
    SET_DIM*sizeof(cart_coordinate),
    cudaMemcpyHostToDevice,
    stream[1]);

  //Make sure stream 1 has finished copying over R before running DR on stream 0
  cudaStreamSynchronize(stream[1]);
  calc_DR_bins<<<grid_dims, block_dims,0,stream[1]>>>
    (d_fake_coords,d_fake_coords,d_rr_bins);


  //Finally, run the DR-calculation
  calc_DR_bins<<<grid_dims, block_dims,0,stream[0]>>>
    (d_real_coords,d_fake_coords,d_dr_bins);

  //Copy over bins
  cudaMemcpyAsync(h_dd_bins,d_dd_bins,
  	BIN_DIM*sizeof(BIN_TYPE),
	cudaMemcpyDeviceToHost);
  cudaMemcpyAsync(h_rr_bins,d_rr_bins,
  	BIN_DIM*sizeof(BIN_TYPE),
	cudaMemcpyDeviceToHost);
  cudaMemcpyAsync(h_dr_bins,d_dr_bins,
  	BIN_DIM*sizeof(BIN_TYPE),
	cudaMemcpyDeviceToHost);

  for (int i = 0; i < 2; ++i)
    cudaStreamDestroy(stream[i]);

  //Calculate result
  double kolm_smir[BIN_DIM];
  calc_kolmogorov(kolm_smir, h_dr_bins, h_dd_bins, h_rr_bins);
  write_kolmogorov("out/distribution.tsv",kolm_smir,BIN_DIM);
  //...free() left out for legibility
\end{lstlisting}
\newpage

\section{Metrics}

Allocations by the compiler:

\begin{verbatim}
	ptxas info    : 0 bytes gmem
	ptxas info    : Compiling entry function '_Z12calc_DR_binsP15cart_coordinateS0_Py'
	        for 'sm_70'
	ptxas info    : Function properties for _Z12calc_DR_binsP15cart_coordinateS0_Py
	    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads
	ptxas info    : Used 22 registers, 5280 bytes smem, 376 bytes cmem[0], 8 bytes cmem[2]
\end{verbatim}

\noindent Unfortunately, timing with \flags{time} is a little unreliable, it depends greatly on the alloted CPU resources (CPU\%).
I think the profiles are better, but I'm not sure.

\vspace{0.5cm}
\noindent Timing with \flags{time},
angles\_per\_thread = 1:
\begin{verbatim} 
	0.37 user
	0.53 system
	0:00.99elapsed
	91%CPU
\end{verbatim}

\vspace{0.5cm}

\noindent angles\_per\_thread = 5:
\begin{verbatim}
	0.18 user
	0.40 system
	0:00.64 elapsed
	91%CPU
\end{verbatim}
\subsubsection{Profiles}
\img{B1_prof}{Profile with angles\_per\_thread = 1}
\img{interleaved_synch}{Profile with angles\_per\_thread = 5}

The second version is faster by far, it appears that letting each thread do more work is better.
Or perhaps the relieved pressure on the internal memory bus is causing the gain in performance.
Here we can see how the host code (green) is interleaved with the kernels (blue).
There is also some small interleaving among the kernels themselves.
I've added the host sections using the NVIDIA Tools Extension library.

\part{Conclusion}

\section{Discussion}

I am happy with the results, the computation takes less than a second (\(\approx\) 640ms).
This was a very fun project which I learned a lot from.
I am now both a better CUDA and C/C++ programmer.
I also understand the GPU mode of computation much better now.

Another thing I enjoyed was profiling my application, and partly competing against my peers in the course.
I am now much more proficient at profiling.

Some things I still wonder about:
A significant portion of this is spent initializing the CUDA context \(\approx\) 390ms!
I wish I could speed this up somehow, but I suspect I can't.
If only we could preseed this context somehow, to save time.
Even though I compiled the program for the sm\_70 (and even sm\_72 just to be sure) architectures, I found calls to JIT-compilers in the profiles.
I don't understand this at all.


\section{Rules of thumb and other lessons learned}

\begin{itemize}
	\item Preprocess data at the scale of the data, instead of processing it at the scale of the work later
	\item Maximize occupancy by keeping warps "intact" (not divergent) by using warp boundaries as conditions for divergent behaviour
	\item Load data from global memory to shared memory before working on it (if many threads in a block need the same data)
	\item {\bf In a kernel}: load all data; synchronize; do work; synchronize; write all data 
	\item Allocate to pinned memory for faster data transfer (only \~{}1 MiB for this assignment, so did not see a significant effect)
	\item Don't parallelize too much, sometimes it's better to have fewer kernels doing more work
	\item Interleave host and device computation as much as possible
	\item There is an interesting dynamic when optimising heterogenous parallel code.
	If a synchronization depends on a procedure completing on both the host and the device, then optimising the faster procedure isn't worth it.
	You have to pull in the two parallel procedures in tandem, or one at a time, always taming the wilder of the two horses.
	\item Finally, gather as much data as you need to understand the optimisation problem.
	Testing different configurations help you develop an intuition for the dynamic at play
	
\end{itemize}

\section{Improvements}

I could still write a calc\_DD\_bins kernel to use the fact that some angles are duplicate in the DD and RR histograms.
I could try to use cudaMemPrefetchAsync or a pointer to the pinned host memory for asynchronous behaviour.
However, since data transfers are cheap for this assignment, it wouldn't make a difference.
I could run the program on several devices and maybe tops double the speed of the kernel execution.
This may however cause a longer initialization sequence, and may ultimately not be worth it.

\end{document}
